# ==== LLM только локальная Ollama ====
LLM_BACKEND=ollama

# Если Ollama поднимается через docker-compose (сервис "ollama"):
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=mistral

# ==== Embeddings (локально, кэшируются в HF_HOME) ====
EMB_MODEL=all-MiniLM-L6-v2

# ==== Сервер приложения ====
HOST=0.0.0.0
PORT=7860